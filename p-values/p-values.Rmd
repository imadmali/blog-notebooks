---
title: "Problems with P-values"
author: "Imad Ali"
date: "6/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

```{r}
clean_theme <- theme(plot.background = element_rect(fill = "#FFFFFF"),
                     panel.background = element_rect(fill = "#FFFFFF", color = "#808080"),
                     # panel.grid.major = element_line(color = alpha("#808080",0.5)),
                     # panel.grid.minor = element_blank(),
                     panel.grid = element_blank(),
                     axis.ticks = element_line(color = alpha("#808080",0.5)),
                     legend.background = element_rect(fill = "#FFFFFF"),
                     legend.key = element_rect(fill = "#FFFFFF"))
```

Two things that have come up often in my experience in industry is how p-values are interpreted (often erroneously) and their behavior under big data. (Because of these issues I try to use alternative approaches to inference.) A great example to work off of to understand these issues is a simple A/B test where we want to compare the treatment group (A) against the control group (B) and evaluate whether there is a "significant" difference between the two. 

## Interpretation

Functions to compute Welch's t-test and associated p-value.

```{r}
driver_welch_ttest <- function(g1_N,
                               g2_N,
                               g1_mean,
                               g2_mean,
                               g1_var,
                               g2_var) {
  g1_var_N <- g1_var / g1_N
  g2_var_N <- g2_var / g2_N
  tstat <- (g1_mean - g2_mean) / sqrt(g1_var_N + g2_var_N)
  denom <- (g1_var_N ^ 2 / (g1_N - 1)) + (g2_var_N ^ 2 / (g2_N - 1))
  degfree <- (g1_var_N + g2_var_N) ^ 2 / denom
  pvalue <- 2 * pt(-abs(tstat), degfree, 0)
  result <- list(tstat = tstat,
                 degfree = degfree,
                 pvalue = pvalue)
  return(result)
}

welch_ttest <- function(g1, g2) {
  g1_mean <- mean(g1)
  g2_mean <- mean(g2)
  g1_N <- length(g1)
  g2_N <- length(g2)
  g1_var <- var(g1)
  g2_var <- var(g2)
  result <- driver_welch_ttest(g1_N, g2_N,
                               g1_mean, g2_mean,
                               g1_var, g2_var)
  class(result) <- "welch_ttest"
  return(result)
}
```

Applying the function above to simulated data.

```{r}
# a <- rnorm(10, 10, 1)
# b <- rnorm(10, 10.5, 1)
a <-
  c(
    11.08954,
    10.57027,
    10.59684,
    8.993971,
    10.4428,
    9.655154,
    11.18748,
    9.881074,
    10.74739,
    11.76134
  )
b <-
  c(
    10.29069,
    9.662908,
    9.794825,
    11.21004,
    10.51539,
    10.49349,
    11.57189,
    10.28759,
    11.35425,
    11.86643
  )
wttest <- welch_ttest(a, b)
wttest
```

Let's compare it with base R's `t.test` function.

```{r}
t.test(a, b, mu = 0)
```

We get the same results.

Below we have a plot method to visualize the p-value result obtained above. We do this by showing the t-distribution probability density function under the null hypothesis (i.e. arbitrary support with degrees of freedom calculated from the data and a non-centrality parameter equal to 0). The test statistic computed above is overlayed on this distribution.

```{r}
plot.welch_ttest <- function(obj) {
  tstat <- abs(obj$tstat)
  # data
  tol <- 3
  lbound <- -tstat - tol
  rbound <- tstat + tol
  support <- seq(lbound, rbound, length.out = 100)
  p <- dt(support, obj$degfree, 0)
  # data frames
  plot_data <- data.frame(support = support, p = p)
  tstat <- data.frame(x = c(-tstat,tstat), name = "tstat")
  # plot
  ttest_plot <- ggplot(data = plot_data) +
    clean_theme +
    theme(legend.title = element_blank()) +
    ggtitle("T-distribution under null hypothesis with test statistics") +
    xlab("Support") + ylab("Density") +
    geom_line(size = 1, aes(x = support, y = p, color = "pdf")) +
    geom_vline(aes(xintercept = x, linetype = name), color = "#0078C8", data = tstat) +
    scale_color_manual(labels = c("pdf"), values = c("#003791"))
    
  return(ttest_plot)
}
```

The p-value defines the sum of the area from the critical value (light blue lines) to the tails of the test statistic's distribution.

```{r}
plot(wttest)
```

Assume that mean and variance are fixed but sample size increases. We show this for several types of variance values.

```{r}
sizes <- seq(5, 100, by = 5)
sd_value <- c(0.5, 1, 3)
sim_pvalues <- list()
i <- 1
for (N in sizes) {
  for (s in sd_value) {
    nr_wttest <- driver_welch_ttest(N, N, 10, 10.5, s, s)
    sim_pvalues[[i]] <- c(sample_size = N, std_dev = s, p_value = nr_wttest$pvalue)
    i <- i + 1
  }
}
sim_pvalues <- do.call("rbind", sim_pvalues)
sim_pvalues <- data.frame(sim_pvalues)
sim_pvalues$std_dev <- factor(sim_pvalues$std_dev)
```

```{r}
ggplot(data = sim_pvalues, aes(x = sample_size, y = p_value, group = std_dev, color = std_dev)) +
  clean_theme + 
  ggtitle("P-value as sample size varies") +
  xlab("Sample Size") + ylab("P-value") +
  ylim(0,1) + 
  scale_color_manual(values=c("#5880C190","#00A08890","#E9546F90"), name = "sd") +
  geom_line(size = 1)
```

## Big Data

Big data can have massive variation (depending on the industry). So let's see what the behavior is like with very large sample sizes and very large standard deviations.

```{r}
sizes <- seq(10, 1e3, by = 10)
sd_value <- c(5, 10, 30)
sim_pvalues <- list()
i <- 1
for (N in sizes) {
  for (s in sd_value) {
    nr_wttest <- driver_welch_ttest(N, N, 10, 10.5, s, s)
    sim_pvalues[[i]] <- c(sample_size = N, std_dev = s, p_value = nr_wttest$pvalue)
    i <- i + 1
  }
}
sim_pvalues <- do.call("rbind", sim_pvalues)
sim_pvalues <- data.frame(sim_pvalues)
sim_pvalues$std_dev <- factor(sim_pvalues$std_dev)
```

```{r}
ggplot(data = sim_pvalues, aes(x = sample_size, y = p_value, group = std_dev, color = std_dev)) +
  clean_theme + 
  ggtitle("P-value as (big data) sample size varies") +
  xlab("Sample Size") + ylab("P-value") +
  ylim(0,1) + 
  scale_color_manual(values=c("#5880C190","#00A08890","#E9546F90"), name = "sd") +
  geom_line(size = 1)
```

So it looks like large sample sizes can also show statistical significance, even if their respective standard deviations are large.

## Simulations

It's unrealistic that the mean and variance would stay exactly the same as the sample size increases. 
Below we run a similar analysis but with simulated data.

```{r}
sizes <- seq(10, 1e3, by = 10)
sd_value <- c(3, 5, 10, 30)
sim_pvalues <- list()
i <- 1
for (N in sizes) {
  for (s in sd_value) {
    pvalue <- c()
    for (n in 1:1e3) {
      x <- rnorm(N, 10, s)
      y <- rnorm(N, 10.5, s)
      nr_wttest <- welch_ttest(x, y)
      # nr_wttest <- t.test(x, y, mu = 0)
      pvalue[n] <- nr_wttest$pvalue
    }
    sim_pvalues[[i]] <- c(sample_size = N, std_dev = s, p_value = mean(pvalue))
    i <- i + 1
  }
}
sim_pvalues <- do.call("rbind", sim_pvalues)
sim_pvalues <- data.frame(sim_pvalues)
sim_pvalues$std_dev <- factor(sim_pvalues$std_dev)
```

```{r}
ggplot(data = sim_pvalues, aes(x = sample_size, y = p_value, group = std_dev, color = std_dev)) +
  clean_theme + 
  ggtitle("P-value as sample size varies") +
  xlab("Sample Size") + ylab("P-value") +
  ylim(0,1) + 
  scale_color_manual(values=c("#C97FB3", "#5880C1","#00A088","#E9546F"), name = "sd") +
  geom_line(size = 1)
```
Heatmap.

```{r}
sizes <- seq(10, 3e3, by = 10)
sd_value <- seq(1, 20, by = 1)
sim_pvalues_heat <- list()
i <- 1
for (N in sizes) {
  for (s in sd_value) {
    pvalue <- c()
    for (n in 1:100) {
      x <- rnorm(N, 10, s)
      y <- rnorm(N, 10.5, s)
      nr_wttest <- welch_ttest(x, y)
      pvalue[n] <- nr_wttest$pvalue
    }
    sim_pvalues_heat[[i]] <- c(sample_size = N, std_dev = s, p_value = mean(pvalue))
    i <- i + 1
  }
}
sim_pvalues_heat <- do.call("rbind", sim_pvalues_heat)
sim_pvalues_heat <- data.frame(sim_pvalues_heat)
# sim_pvalues_heat$std_dev <- factor(sim_pvalues_heat$std_dev)
```

```{r}
ggplot(data = sim_pvalues_heat, aes(x = sample_size, y = std_dev, fill = p_value)) +
  clean_theme +
  ggtitle("P-values by sample size and variance") +
  xlab("Sample Size") + ylab("Standard Deviation") +
  scale_fill_gradientn(colors = c("#003791","#99afd3")) +
  geom_raster() 
```

Delete next two paragraphs.

We come to a similar conclusion. P-values tend to decrease as sample sizes increase. This is behavior is intended of p-values, and the practitioner should be aware of it when making inferences.

## Alternative Inference

Since p-values can be misleading it is a good perform alternative inference. the approaches discussed here are Bayesian appraoches.

### Overlap coefficient

### Bhattacharyya coefficient

### K-L Divergence

The approach more of a parametric aproach compared to the the previous two.

